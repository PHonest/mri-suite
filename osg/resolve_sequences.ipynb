{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f68c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "additional_paths = [\n",
    "    \"/home/homesOnMaster/pehrlich/dataset_preparation\",\n",
    "    \"/home/homesOnMaster/pehrlich/dataset_preparation/lib\",\n",
    "]\n",
    "\n",
    "# Add the parent folder (adjust the path as needed)\n",
    "for _path in additional_paths:\n",
    "    parent_dir = os.path.expanduser(_path)\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99d07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lib.metadata import StudyMetadata, Series\n",
    "from lib.utils import flatten, remove_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f816bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homesOnMaster/pehrlich/miniforge3/envs/shoulder/lib/python3.13/site-packages/pydicom/valuerep.py:440: UserWarning: Invalid value for VR UI: '1.2.840.113619.2.94.1617958472523.021932225413.32890.9938347'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warn_and_log(msg)\n",
      "/home/homesOnMaster/pehrlich/miniforge3/envs/shoulder/lib/python3.13/site-packages/pydicom/valuerep.py:440: UserWarning: Invalid value for VR UI: '1.2.840.113619.2.94.1694695898880.021932225413.13107.1381128'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warn_and_log(msg)\n"
     ]
    }
   ],
   "source": [
    "study_metadata: dict[str, StudyMetadata] = np.load(\n",
    "    \"/home/homesOnMaster/pehrlich/dataset_preparation/osg/study_map_osg.npy\",\n",
    "    allow_pickle=True\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(data, labels, title, xlabel, ylabel=\"Count\", color='skyblue', figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plots a histogram with given data and labels.\n",
    "    \n",
    "    Args:\n",
    "        data: List of counts for each group\n",
    "        labels: List of labels for each group\n",
    "        title: Title for the plot\n",
    "        xlabel: Label for x-axis\n",
    "        ylabel: Label for y-axis (default: \"Count\")\n",
    "        color: Color for the bars (default: 'skyblue')\n",
    "        figsize: Figure size tuple (default: (10, 6))\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    bars = plt.bar(labels, data, color=color)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for bar, count in zip(bars, data):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(data)*0.01, \n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b35bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ignore_patterns = [\n",
    "    r\"(?i).*befund.*\",\n",
    "    r\"(?i).*survey.*\",\n",
    "    r\"(?i).*localizer.*\",\n",
    "    r\"(?i).*iso.*\",\n",
    "    r\"(?i).*MPR.*\",\n",
    "]\n",
    "\n",
    "series_matching_patterns = {\n",
    "    \"cor_t1\": [\n",
    "        r\"(?i).*t1_tse_cor(?!.*km).*\",\n",
    "        r\"(?i)(?=.*t1w)(?=.*cor)(?=.*tse)(?!.*km).*\",\n",
    "    ],\n",
    "    \"cor_t1_km\": [\n",
    "        r\"(?i)(?=.*t1)(?=.*cor)(?=.*km).*\",\n",
    "        r\"(?i)(?=.*t1)(?=.*cor)(?=.*km).*\",\n",
    "    ],\n",
    "    \"cor_pd\": [\n",
    "        r\"(?i)(?=.*pd)(?=.*cor)(?!.*km).*\",\n",
    "        r\"(?i).*PDW_SPIR_COR.*\",\n",
    "    ],\n",
    "    \"sag_t1\": [\n",
    "        r\"(?i).*t1_tse_sag_DRB.*\",\n",
    "        r\"(?i).*T1W_TSE_SAG.*\",\n",
    "        r\"(?i).*T1W_aTSE_SAG.*\",\n",
    "        r\"(?i).*t1_tse_sag.*\",\n",
    "    ],\n",
    "    \"sag_t1_km\": [\n",
    "        r\"(?i)(?=.*t1)(?=.*sag)(?=.*km).*\",\n",
    "        r\"(?i)(?=.*t1)(?=.*sag)(?=.*km).*\",\n",
    "    ],\n",
    "    \"sag_pd\": [\n",
    "        r\"(?i)(?=.*pd)(?=.*sag)(?!.*km).*\",\n",
    "        r\"(?i).*PDW_SPIR_SAG.*\",\n",
    "        r\"(?i).*PDW_SPIR sag.*\",\n",
    "    ],\n",
    "    \"tra_t1_km\": [\n",
    "        r\"(?i)(?=.*t1)(?=.*tra)(?=.*km).*\",\n",
    "        r\"(?i)(?=.*t1)(?=.*tra)(?=.*km).*\",\n",
    "    ],\n",
    "    \"tra_t2\": [\n",
    "        r\"(?i).*t2_tse_tra.*\",\n",
    "        r\"(?i)(?=.*t2)(?=.*tra)(?=.*tse)(?!.*km).*\",\n",
    "    ],\n",
    "    \"tra_pd\": [\n",
    "        r\"(?i)(?=.*pd)(?=.*tra)(?!.*km).*\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "relevant_mods = [\"tra_t2\", \"sag_pd\", \"cor_pd\", \"cor_t1\"]\n",
    "\n",
    "def contains_pattern(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_matches_in_modality(series_counts):\n",
    "    tmp_counts = deepcopy(series_counts)\n",
    "    mod_matches = []\n",
    "\n",
    "    for mod, rules in series_matching_patterns.items():\n",
    "        matches = flatten([list(filter(lambda x: contains_pattern(x[0], [rule]), tmp_counts)) for rule in rules])\n",
    "        matches = remove_dupes(matches)\n",
    "        mod_matches.extend(matches)\n",
    "        print(f\"{mod}: Found {len(matches)} rules matching {sum([x[1] for x in matches])}\")\n",
    "\n",
    "    return mod_matches\n",
    "\n",
    "def show_matches_in_modality(series_counts: list[tuple[int,int]], mod: str):\n",
    "    tmp_counts = deepcopy(series_counts) \n",
    "    matches = flatten([list(filter(lambda x: contains_pattern(x[0], [rule]), tmp_counts)) for rule in series_matching_patterns[mod]])\n",
    "    matches = remove_dupes(matches)\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"{mod}: Found {len(matches)} rules matching {sum([x[1] for x in matches])}\")\n",
    "    for desc, count in sorted_matches:\n",
    "        print(f\"  {desc}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_description_counts = {}\n",
    "failed_series_descriptions = 0\n",
    "total_series = 0\n",
    "\n",
    "for study in study_metadata.values():\n",
    "    try:\n",
    "        for series_uid, series in study[\"series\"].items():\n",
    "            total_series += 1\n",
    "            description = series.get(\"series_description\")\n",
    "            if description:\n",
    "                series_description_counts[description] = series_description_counts.get(description, 0) + 1\n",
    "            else:\n",
    "                series_description_counts['Unknown'] = series_description_counts.get('Unknown', 0) + 1\n",
    "    except (TypeError, KeyError):\n",
    "        failed_series_descriptions += 1\n",
    "\n",
    "# Sort series descriptions by count (descending)\n",
    "sorted_series_descriptions = sorted(series_description_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Total number of series: {total_series}\")\n",
    "print(f\"Number of different series descriptions: {len(series_description_counts)}\")\n",
    "print(f\"Failed to parse series descriptions for {failed_series_descriptions} studies\")\n",
    "print(\"\\nTop 20 series descriptions:\")\n",
    "for desc, count in sorted_series_descriptions[:20]:\n",
    "    print(f\"  {desc}: {count}\")\n",
    "\n",
    "# Plot top 20 series descriptions\n",
    "top_20_series = sorted_series_descriptions[:15]\n",
    "descriptions = [desc[0] for desc in top_20_series]\n",
    "counts = [desc[1] for desc in top_20_series]\n",
    "\n",
    "plot_histogram(\n",
    "    data=counts,\n",
    "    labels=descriptions,\n",
    "    title=\"Top 15 Series Descriptions by Frequency\",\n",
    "    xlabel=\"Series Description\",\n",
    "    color='mediumpurple',\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_series_counts: list[tuple[str, int]] = []\n",
    "for desc, count in series_description_counts.items():\n",
    "    if contains_pattern(desc, patterns= ignore_patterns): continue\n",
    "    filtered_series_counts.append((desc, count))\n",
    "\n",
    "sorted_series_counts = sorted(filtered_series_counts, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a219707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unmatched series descriptors\n",
    "unmatched_series_counts = deepcopy(sorted_series_counts)\n",
    "\n",
    "for _, rules in series_matching_patterns.items():\n",
    "    unmatched_series_counts = list(filter(lambda x: not contains_pattern(x[0], rules), unmatched_series_counts))\n",
    "\n",
    "\n",
    "matches = calculate_matches_in_modality(sorted_series_counts)\n",
    "\n",
    "assert sum([x[1] for x in matches]) < total_series\n",
    "\n",
    "print(f\"Matched {sum([x[1] for x in matches])} from series {sum([x[1] for x in sorted_series_counts])}\")\n",
    "print(f\"Still found {len(unmatched_series_counts)} unmatched descriptions and {sum([x[1] for x in unmatched_series_counts])} unmatched series\")\n",
    "for desc, count in unmatched_series_counts[:20]:\n",
    "    print(f\"  {desc}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_matches = {}\n",
    "\n",
    "for mod, rules in series_matching_patterns.items():\n",
    "    matches = flatten([list(filter(lambda x: contains_pattern(x[0], [rule]), sorted_series_counts)) for rule in rules])\n",
    "    matches = remove_dupes(matches)\n",
    "    modality_matches[mod] = sum([x[1] for x in matches])\n",
    "\n",
    "modalities = list(modality_matches.keys())\n",
    "counts = list(modality_matches.values())\n",
    "\n",
    "plot_histogram(\n",
    "    data=counts,\n",
    "    labels=modalities,\n",
    "    title=\"Series Matches by Modality\",\n",
    "    xlabel=\"Modality\",\n",
    "    ylabel=\"Number of Matched Series\",\n",
    "    color='lightcoral',\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "\n",
    "print(f\"Total matched series: {sum(counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b3919",
   "metadata": {},
   "source": [
    "## Improve sequence mapping - Find incomplete studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find studies which do not include all major sequences \n",
    "def get_modality_series(modality: str, study_metadata: StudyMetadata) -> list[Series]: \n",
    "    matched_series = []\n",
    "    for series in study_metadata[\"series\"].values():\n",
    "        if contains_pattern(series[\"series_description\"], series_matching_patterns[modality]):\n",
    "            matched_series.append(series)\n",
    "\n",
    "    return matched_series\n",
    "\n",
    "studies_with_missing_sequences: dict[str, list[str]] = {}\n",
    "studies_with_found_sequences: dict[str, dict[str, str]] = {}\n",
    "\n",
    "for study in list(study_metadata.values()):\n",
    "    for mod in relevant_mods:\n",
    "\n",
    "        series = get_modality_series(mod, study)\n",
    "        if not series:\n",
    "            if study[\"study_instance_uid\"] not in studies_with_missing_sequences:\n",
    "                studies_with_missing_sequences[study[\"study_instance_uid\"]] = [mod]\n",
    "            else:\n",
    "                studies_with_missing_sequences[study[\"study_instance_uid\"]].append(mod)\n",
    "\n",
    "        else:\n",
    "            if study[\"study_instance_uid\"] not in studies_with_found_sequences:\n",
    "                studies_with_found_sequences[study[\"study_instance_uid\"]] = {}\n",
    "            \n",
    "            studies_with_found_sequences[study[\"study_instance_uid\"]][mod] = [s[\"series_description\"] for s in series]\n",
    "            \n",
    "\n",
    "print(f\"Found {len(studies_with_missing_sequences)} studies with missing sequences\")\n",
    "print(studies_with_missing_sequences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd05a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create document find missing sequences\n",
    "study_correct = []\n",
    "\n",
    "for study_uid, sequences in studies_with_missing_sequences.items():\n",
    "\n",
    "    study_correct.append({\n",
    "        \"StudyInstanceUID\": study_uid,\n",
    "        **{mod: None for mod in sequences},\n",
    "        **studies_with_found_sequences.get(study_uid, {}),\n",
    "        \"AvailableDescriptions\": [s[\"series_description\"] for s in study_metadata[study_uid][\"series\"].values() if not contains_pattern(s[\"series_description\"], ignore_patterns)]\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "column_order = [\n",
    "    \"StudyInstanceUID\",\n",
    "    \"tra_t2\", \n",
    "    \"sag_pd\", \n",
    "    \"cor_pd\", \n",
    "    \"cor_t1\",\n",
    "    \"AvailableDescriptions\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(study_correct, columns=column_order)\n",
    "df.to_csv(\"studies_with_missing_sequences.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f38205",
   "metadata": {},
   "source": [
    "## Improve sequence mapping - Find redundant studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccfa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Improve sequence mapping - Find redundant studies\n",
    "\n",
    "studies_with_redundant_sequences: dict[str, list[str]] = {}\n",
    "studies_with_redundant_sequences_series: dict[str, list[Series]] = {}\n",
    "studies_with_clear_sequences: dict[str, dict[str, str]] = {}\n",
    "\n",
    "for study in list(study_metadata.values()):\n",
    "    if study[\"study_instance_uid\"] in studies_with_missing_sequences.keys():\n",
    "        continue\n",
    "\n",
    "    for mod in relevant_mods:\n",
    "\n",
    "        series = get_modality_series(mod, study)\n",
    "        if len(series) > 1: \n",
    "            if study[\"study_instance_uid\"] not in studies_with_redundant_sequences:\n",
    "                studies_with_redundant_sequences[study[\"study_instance_uid\"]] = [mod]\n",
    "                studies_with_redundant_sequences_series[study[\"study_instance_uid\"]] = [series]\n",
    "            else:\n",
    "                studies_with_redundant_sequences[study[\"study_instance_uid\"]].append(mod)\n",
    "                studies_with_redundant_sequences_series[study[\"study_instance_uid\"]].append(series)\n",
    "\n",
    "        else:\n",
    "            if study[\"study_instance_uid\"] not in studies_with_clear_sequences:\n",
    "                studies_with_clear_sequences[study[\"study_instance_uid\"]] = {}\n",
    "            \n",
    "            studies_with_clear_sequences[study[\"study_instance_uid\"]][mod] = [s[\"series_description\"] for s in series]\n",
    "            \n",
    "\n",
    "print(f\"Found {len(studies_with_redundant_sequences)} studies with missing sequences\")\n",
    "print(studies_with_redundant_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG CELL\n",
    "# print(list(studies_with_redundant_sequences_series.keys()))\n",
    "# print([list(map(lambda x: x[\"series_instance_uid\"], series)) for series in studies_with_redundant_sequences_series.values()])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create document find missing sequences\n",
    "study_correct = []\n",
    "\n",
    "for (study_uid, modalities), series in zip(studies_with_redundant_sequences.items(), studies_with_redundant_sequences_series.values()):\n",
    "    _study_correct = {\"StudyInstanceUID\": study_uid}\n",
    "\n",
    "    for _mod, _series in zip(modalities, series):\n",
    "        _study_correct[_mod] = [s[\"series_instance_uid\"] for s in _series]\n",
    "\n",
    "    study_correct.append(_study_correct)\n",
    "\n",
    "\n",
    "column_order = [\n",
    "    \"StudyInstanceUID\",\n",
    "    \"tra_t2\", \n",
    "    \"sag_pd\", \n",
    "    \"cor_pd\", \n",
    "    \"cor_t1\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(study_correct, columns=column_order)\n",
    "df.to_csv(\"studies_with_redundant_sequences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc05600",
   "metadata": {},
   "source": [
    "## Extract interesting studies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98453fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ['1.2.840.113619.6.95.31.0.3.4.1.3096.13.361415', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.361585', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.245827', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.251187', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.262522', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.251182', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.255552', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.362128', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.260426', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.263561', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.243674', '1.2.840.113619.6.95.31.0.3.4.1.3096.13.255578']\n"
     ]
    }
   ],
   "source": [
    "interesting_studies = []\n",
    "\n",
    "km_studies = list(filter(lambda x: x[\"with_km\"], study_metadata.values()))\n",
    "low_fs_studies = list(filter(lambda x: x[\"magnetic_field_strength\"] == 0.55, study_metadata.values()))\n",
    "mid_fs_studies = list(filter(lambda x: x[\"magnetic_field_strength\"] == 1.5, study_metadata.values()))\n",
    "high_fs_studies = list(filter(lambda x: x[\"magnetic_field_strength\"] == 3, study_metadata.values()))\n",
    "\n",
    "studies = [*km_studies[:3],*low_fs_studies[:3], *mid_fs_studies[:3], *high_fs_studies[:3]]\n",
    "study_uids = set(map(lambda x: x[\"study_instance_uid\"], studies))\n",
    "print(len(study_uids), list(study_uids))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoulder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
